---
output:
  html_document:
    df_print: paged
---

```{r}
                        ###############################################################
                        #    ____                 ______                          __  #
                        #   / __/_ _____  ___ ___/_  __/______ ____  ___ ___ ____/ /_ #
                        #  _\ \/ // / _ \/ -_) __// / / __/ _ `/ _ \(_-</ -_) __/ __/ #
                        # /___/\_,_/ .__/\__/_/  /_/ /_/  \_,_/_//_/___/\__/\__/\__/  #
                        #         /_/                                                 #
                        ###############################################################
```

Welcome to the SuperTransect. 

This is Illumina Miseq data looking at 16S (V4 region, primers 515/806R)
Several sample types were sampled along 8 transects in Waimea Valley. 
Each sample was duplicated for DNA extraction and metabolomics. 

# Metadata Overview
Here's a quick breakdown of the metadata columns:

**sample_barcode** is the unique identifier for each sample.

These are all related to where the sample was collected. A little messy.

* **site_code**
* **transect_name**
* **site_name**
* **site_type**
* **distance** 

**lat** and **long** are in decimal degrees. Be careful, though, because some samples (like mosquitoes) have unique lat long decoupled from collection site.

These are all related to the sample type (e.g. plants vs. water)

* **sample_type**
* **host** 
* **habitat**
* **trophic** 

**analysis** indicated whether the sample was used for metabolomics (MS), dna extraction (DNA), or was split in the lab (MSandDNA).

**processing and notes** are mostly for sample management or any additional metadata (e.g. species names). Look at these if you need to go deeper.


```{r}
suppressWarnings(library(dplyr))
library(tidyr)
library(stringr)

file_metadata <- "../../data/raw/SuperTransect_All_Samples - all_collected_samples.csv"
metadata <- read.csv(file_metadata , header = T,colClasses = "character")
metadata <- metadata %>% select(-processing,-notes) %>% arrange(sample_id)

# Column names
metadata
```
### Transects

We sampled 8 transects in the Waimea watershed ranging from the the summit of the watershed, Kainapuaa (STSummit), to the beach of Waimea Bay (STEstuary).
```{r}
transects <- metadata %>% filter(site_type == "Transect") %>% arrange(lat)
unique(transects$transect_name) %>% as.character()

```

Each transect had 9 sampling points, spaced 100m apart. These can be accessed in the **distance** column.
The **site_code** column replicates this information. **site_name** is the unique combination of transect name and distance.

```{r}
# distances
unique(transects$distance) %>% as.character()

# site codes
unique(transects$site_code) %>% as.character()

# site names
unique(transects$site_name) %>% as.character()
```

### Marine Sites

The four marine sites were sampled the same week as the transects. However, they are not transects and do not have any 'distance' structure.
The sampling at these sites followed a bioblitz format. Host species are not necessarily the same across sites.
Because there is no distance, site name and site code are the same.


```{r}
marine <- metadata %>% filter(site_type == "Core Waimea")
# site name
unique(marine$site_name) %>% as.character()
# site code
unique(marine$site_code) %>% as.character()

```


### 16S Data Quality Overview - Reads per step

Before we read in the whole ASV table, let's look at the reads per step to see how read attrition looks in the pipeline.
If read attrition is very high for a certain step, we might need to adjust pipeline parameters.
For example, high attrition at the 'ESV' step indicates poor read pairing. 
Adjusting trimming and mismatch requirements could help with this

Alternatively, losing a lot of reads at the taxa filter step indicates problems with host amplification.
The only way to recover these samples is to lower the subsampling threshold. 

It's important to note that sequencing id is the unique identifier for a sequenced sample.
Sample barcode is the unique id for the collected sample. Collected samples can be sequenced multiple times.
A given sample barcode may be associated with multiple sequencing ids. 

We'll read in the full mapping file. The mapping file has all sample collection metadata.
We can join the mapping file with the run map based on sample barcode.
With this information, we can break down reads per step by sample type. 

```{r}

# run map pairs barcode reads with unique sequencing id, run info, and primer info. 
file_run_map <- "../../data/raw/st_miseq04_mapping_file - run_map.csv"
run_map <- read.csv(file_run_map, header = T, colClasses = "character")
run_map[1] <- as.numeric(run_map[[1]])


file_read_step <- "../../data/interim/sequences_per_sample_per_step_97.tsv"
read_step <- read.table(file_read_step, sep = "\t",header = T)
read_step <- apply(read_step, 2, function(x) sub(" \\((.+) uniques\\)","",x))
read_step <- apply(read_step[], 2, function(x) as.numeric(x)) %>% as.data.frame()
colnames(read_step)[1] <- "id"
# join reads per step with run info and sample metadata
read_step <-
  read_step %>% left_join(run_map, by = "id") %>% left_join(metadata, by = "sample_barcode")

# read_step now contains run info, reads per step for each sequencing id, and sample metadata.
read_step

```

### Summarize reads per step by sample type
It looks like most read attrition is happening at the taxa filtering step.
Probably, this is an issue with host amplification.


```{r}
read_summary <-
  read_step %>% mutate(pass = ifelse(is.na(X12.Subsampling_97), "Fail", "Pass"))

read_summary <-
  read_summary %>% count(pass, sample_type) %>% pivot_wider(names_from = pass, values_from = n) %>% arrange(desc(Fail))

sum(read_summary$Fail, na.rm = T)
sum(read_summary$Pass, na.rm = T)

read_summary

# write out summary of samples passing pipeline
write.csv(read_summary,"outputs/samples_passing_pipeline.csv")
write.csv(read_step, "outputs/reads_per_step_w_meta.csv")

```

### Adjust sub-sampling
As expected, most of the failed samples are plants, presumably because of high host chloroplast amplification rates. 
There are still reads in these samples, but we'll need to adjust subsampling if we want to include these samples.
Let's check the plant reads to determine optimal subsampling


```{r}
# histogram plant samples
plants <- read_step %>% filter(sample_type %in% c("PlantShoot","PlantRoot"))
plants$X10.TaxaFilter_97[plants$X10.TaxaFilter_97 < 5000] %>% as.numeric() %>% hist(breaks = 20, main = "Plant samples under 5000 reads")

# count samples kept based on different cut off levels

cut_check <- data.frame()

for (cutoff in c(0,2000,2500,3000,3400,3500,3700,4000)) {
cuts <- read_step %>% group_by(sample_type) %>% filter(X10.TaxaFilter_97 > cutoff)
cuts <- mutate(cuts, Cutoff = cutoff)

cut_check <- rbind.data.frame(cut_check, cuts)

}

cut_check %>% group_by(Cutoff, sample_type) %>% count() %>% pivot_wider(names_from = Cutoff, values_from = n)

```

So it looks like plant shoots are not going to work out. Almost all of the samples have <2000 reads after taxa filtering.
Plant roots look okay. If we cut off at 3700, we keep 5 more samples than at 4000 and only lose 3 samples compared to 3000 (2 of which are plant shoots, which will be excluded)

## Import Pipeline Outputs (taxonomy, metadata, ESV table)

```{r}
# source scripts for cleaning pipeline output and making phyloseq object
source("src/clean_and_query_16S.R")
source("src/make_phyloseq.R")


ST_16S <- clean_16S_tables(abundance_file = "../../data/interim/abundance_table_97.shared",
                  taxonomy_file = "../../data/interim/annotations_97.taxonomy",
                 metadata_file = "../../data/raw/st_miseq04_mapping_file - mapping_new.csv",
                 description = "ST_16S_Miseq04")

head(ST_16S$abundance)
head(ST_16S$taxonomy)
head(ST_16S$metadata)

ST_16S_phy <- make_phyloseq(ST_16S)



```


