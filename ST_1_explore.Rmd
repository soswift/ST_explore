---
title: "ST_1_explore"
author: "Sean Swift"
date: "January 30, 2020"
output: html_document:
        toc: true
        toc_depth: 4
        number_sections: true
---

# Supertransect
Welcome to the SuperTransect. 

This is Illumina Miseq data looking at 16S (V4 region, primers 515/806R)
Several sample types were sampled along 8 transects in Waimea Valley. 
Each sample was duplicated for DNA extraction and metabolomics. 

This R markdown is for checking the metadata, importing 16S pipeline outputs, and exporting cleaned data.


# Metadata Overview
Here's a quick breakdown of the metadata columns:

**sample_barcode** is the unique identifier for each sample.


Site metadata columns

* **site_code** sites 1-9 for each transect, separated by 100m (as measured by GPS)
* **transect_name** 
* **site_name**
* **site_type**
* **site_order** ordered by elevation (marine sites up to summit)

**lat** and **long** are in decimal degrees. Be careful, though, because some samples (like mosquitoes) have unique lat long decoupled from collection site.

Sample metadata columns

* **sample_type**
* **host** 
* **habitat**
* **trophic** 

**analysis** indicates whether the sample was used for metabolomics (MS), dna extraction (DNA), or was split in the lab (MSandDNA).

**processing and notes** are mostly for sample management or any additional metadata (e.g. species names). Look at these if you need to go deeper.


```{r}
setwd("~/Documents/Bioinformatics/Projects/SuperTransect/Analysis/exploration/R_notebook/")
suppressPackageStartupMessages(library(dplyr))
library(tidyr)
library(stringr)
library(ggplot2)

file_metadata <- "data/raw/sample/st_miseq04_mapping_file - run_meta_map.csv"

metadata      <- read.csv(file_metadata , header = T)
metadata      <- metadata %>% select(-processing,-notes) %>% arrange(sample_id)

# small preview
metadata[1:5,1:5]
```
## Transects

We sampled 8 transects in the Waimea watershed ranging from the the summit of the watershed, Kainapuaa (STSummit), to the beach of Waimea Bay (STEstuary).
```{r}
transects <- metadata %>% filter(site_type == "Transect") %>% arrange(lat)
unique(transects$transect_name) %>% as.character()

```

Each transect had 9 sampling points, spaced 100m apart. These can be accessed in the **distance** column.
The **site_code** column replicates this information. **site_name** is the unique combination of transect name and distance.

```{r}
# site codes
unique(transects$site_code) %>% as.character()

# site names
unique(transects$site_name) %>% as.character()
```

## Marine Sites

The four marine sites were sampled the same week as the transects. However, they are not transects and do not have any 'distance' structure.
The sampling at these sites followed a bioblitz format. Host species are not necessarily the same across sites.
Because there is no distance, site name and site code are the same.


```{r}
marine <- metadata %>% filter(site_type == "Core Waimea")
# site name
unique(marine$site_name) %>% as.character()
# site code
unique(marine$site_code) %>% as.character()

```

## Replication across sample types

```{r}

# typical end site
print("Transect Start/Middle/End Sites. 0m, 400m, 800m.")
transects %>% filter(site_name == "STGardens9") %>% group_by(site_name, sample_type) %>% summarise(sample_counts = n())

# typical transect site 2:4,6:8
print("Transect Filler Sites. 100m, 200m, 300m, 500m, 600m, 700m.")
transects %>% filter(site_name == "STGardens2") %>% group_by(site_name, sample_type) %>% summarise(sample_counts = n())



```

# Sequencing Data
## 16S Data Cleaning - Reads per step and removing failed samples

Before we read in the whole ASV table, let's look at the reads per step to see how read attrition looks in the pipeline.
If read attrition is very high for a certain step, we might need to adjust pipeline parameters.
For example, high attrition at the 'ESV' step indicates poor read pairing. 
Adjusting trimming and mismatch requirements could help with this

Alternatively, losing a lot of reads at the taxa filter step indicates problems with host amplification.
The only way to recover these samples is to lower the subsampling threshold. 

It's important to note that sequencing id is the unique identifier for a sequenced sample.
Sample barcode is the unique id for the collected sample. Collected samples can be sequenced multiple times.
A given sample barcode may be associated with multiple sequencing ids. 

We'll read in the full mapping file. The mapping file has all sample collection metadata.
We can join the mapping file with the run map based on sample barcode.
With this information, we can break down reads per step by sample type. 

```{r}

# run map pairs barcode reads with unique sequencing id, run info, and primer info. 
file_run_map <- "data/raw/sample/st_miseq04_mapping_file - run_meta_map.csv"
run_map <- read.csv(file_run_map, header = T, colClasses = "character")
run_map[1] <- as.numeric(run_map[[1]])


file_read_step <- "data/raw/sequence/sequences_per_sample_per_step_97.tsv"
read_step <- read.table(file_read_step, sep = "\t",header = T)
read_step <- apply(read_step, 2, function(x) sub(" \\((.+) uniques\\)","",x))
read_step <- apply(read_step[], 2, function(x) as.numeric(x)) %>% as.data.frame()
colnames(read_step)[1] <- "id"
# join reads per step with run info and sample metadata
read_step <-
  read_step %>% left_join(run_map, by = "id")

# read_step now contains run info, reads per step for each sequencing id, and sample metadata.

```

## Summarize reads per step by sample type
It looks like most read attrition is happening at the taxa filtering step.
Probably, this is an issue with host amplification.


```{r}
read_summary <-
  read_step %>% mutate(pass = ifelse(is.na(X12.Subsampling_97), "Fail", "Pass"))

read_summary <-
  read_summary %>% count(pass, sample_type) %>% pivot_wider(names_from = pass, values_from = n) %>% arrange(desc(Fail))

sum(read_summary$Fail, na.rm = T)
sum(read_summary$Pass, na.rm = T)



# write out summary of samples passing pipeline
write.csv(read_summary,"data/processed/samples_passing_pipeline.csv")
write.csv(read_step, "data/processed/reads_per_step_w_meta.csv")

head(read_summary)

```

### Adjust sub-sampling
As expected, most of the failed samples are plants, presumably because of high host chloroplast amplification rates. 
There are still reads in these samples, but we'll need to adjust subsampling if we want to include these samples.
Let's check the plant reads to determine optimal subsampling


```{r}
# count samples kept based on different cut off levels

cut_check <- data.frame()

for (cutoff in c(0,2000,2500,3000,3400,3500,3700,4000)) {
cuts <- read_step %>% group_by(sample_type) %>% filter(X10.TaxaFilter_97 > cutoff)
cuts <- mutate(cuts, Cutoff = cutoff)

cut_check <- rbind.data.frame(cut_check, cuts)

}

cut_check<- cut_check %>% group_by(Cutoff, sample_type) %>% count() %>% pivot_wider(names_from = Cutoff, values_from = n)
head(cut_check)
```

So it looks like plant shoots are not going to work out. Almost all of the samples have <2000 reads after taxa filtering.
Plant roots look okay. If we cut off at 3700, we keep 5 more samples than at 4000 and only lose 3 samples compared to 3000 (2 of which are plant shoots, which will be excluded)

## Import Pipeline Outputs (taxonomy, metadata, ESV table), clean, export

```{r}
# source scripts for cleaning pipeline output and making phyloseq object
source("src/clean_and_query_16S.R")


## clean_16S_tables outputs a list of cleaned versions of each dataframe
## 'cleaning' means renaming some columns and generally tidying the pipeline outputs
## sequences are not affected
## names are checked to make sure they agree between all files
## the function writes out each cleaned file as a csv

ST_16S <- clean_16S_tables(abundance_file = "data/raw/sequence/abundance_table_97.shared",
                  taxonomy_file = "data/raw/sequence/annotations_97.taxonomy",
                 metadata_file = "data/raw/sample/st_miseq04_mapping_file - run_meta_map.csv",
                 description = "ST_16S_Miseq04",
                 output_dir = "data/processed/cleaned/")

## save list of dataframes as an r object to make importing easier

saveRDS(ST_16S, "data/processed/cleaned/ST_16S.rds")



```
